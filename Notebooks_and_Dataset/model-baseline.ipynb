{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5611364,"sourceType":"datasetVersion","datasetId":3225723},{"sourceId":8427130,"sourceType":"datasetVersion","datasetId":5017962}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport time\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom torchvision import transforms\nfrom typing import Dict, Tuple\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-16T05:15:06.507162Z","iopub.execute_input":"2024-05-16T05:15:06.507575Z","iopub.status.idle":"2024-05-16T05:15:15.473845Z","shell.execute_reply.started":"2024-05-16T05:15:06.507540Z","shell.execute_reply":"2024-05-16T05:15:15.472870Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"final_train_df = pd.read_csv(\"/kaggle/input/vqa-filtered/textpreprocessed_labelencoding.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:15.475663Z","iopub.execute_input":"2024-05-16T05:15:15.476148Z","iopub.status.idle":"2024-05-16T05:15:16.065414Z","shell.execute_reply.started":"2024-05-16T05:15:15.476121Z","shell.execute_reply":"2024-05-16T05:15:16.064387Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"final_train_df","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:16.067117Z","iopub.execute_input":"2024-05-16T05:15:16.067734Z","iopub.status.idle":"2024-05-16T05:15:16.096663Z","shell.execute_reply.started":"2024-05-16T05:15:16.067694Z","shell.execute_reply":"2024-05-16T05:15:16.095564Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                image_id  question_id  \\\n0      /kaggle/input/visual-question-answering/train2...    262146000   \n1      /kaggle/input/visual-question-answering/train2...    262146001   \n2      /kaggle/input/visual-question-answering/train2...    262146002   \n3      /kaggle/input/visual-question-answering/train2...    393221000   \n4      /kaggle/input/visual-question-answering/train2...    393221001   \n...                                                  ...          ...   \n96886  /kaggle/input/visual-question-answering/train2...    393195000   \n96887  /kaggle/input/visual-question-answering/train2...    393195001   \n96888  /kaggle/input/visual-question-answering/train2...    393195002   \n96889  /kaggle/input/visual-question-answering/train2...    262136000   \n96890  /kaggle/input/visual-question-answering/train2...    262136002   \n\n                                 question_preprocessed       question_type  \\\n0                               what color is the snow   what color is the   \n1                             what is the person doing  what is the person   \n2                   what color is the persons headwear   what color is the   \n3                                      is the sky blue              is the   \n4                       is there snow on the mountains            is there   \n...                                                ...                 ...   \n96886                               why is she smiling                 why   \n96887                       is the woman wearing a hat        is the woman   \n96888                     is the woman wearing glasses        is the woman   \n96889                      can he be sharpening blades   none of the above   \n96890  is this a normal thing to see in recent history           is this a   \n\n      answer_preprocessed                                            answers  \\\n0                   white  ['white', 'white', 'white', 'white', 'white', ...   \n1                  skiing  ['skiing', 'skiing', 'skiing', 'skiing', 'skii...   \n2                     red  ['red', 'red', 'red & white', 'black', 'red', ...   \n3                     yes  ['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...   \n4                     yes  ['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...   \n...                   ...                                                ...   \n96886               happy  ['heard joke', 'happy', \"she's enjoying her bo...   \n96887                 yes  ['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...   \n96888                 yes  ['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...   \n96889                 yes  ['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...   \n96890                  no  ['no', 'no', 'no', 'yes', 'no', 'no', 'no', 'n...   \n\n      answer_type  answer_encoded  \n0           other             959  \n1           other             791  \n2           other             708  \n3          yes/no             995  \n4          yes/no             995  \n...           ...             ...  \n96886       other             419  \n96887      yes/no             995  \n96888      yes/no             995  \n96889      yes/no             995  \n96890      yes/no             559  \n\n[96891 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>question_id</th>\n      <th>question_preprocessed</th>\n      <th>question_type</th>\n      <th>answer_preprocessed</th>\n      <th>answers</th>\n      <th>answer_type</th>\n      <th>answer_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>262146000</td>\n      <td>what color is the snow</td>\n      <td>what color is the</td>\n      <td>white</td>\n      <td>['white', 'white', 'white', 'white', 'white', ...</td>\n      <td>other</td>\n      <td>959</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>262146001</td>\n      <td>what is the person doing</td>\n      <td>what is the person</td>\n      <td>skiing</td>\n      <td>['skiing', 'skiing', 'skiing', 'skiing', 'skii...</td>\n      <td>other</td>\n      <td>791</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>262146002</td>\n      <td>what color is the persons headwear</td>\n      <td>what color is the</td>\n      <td>red</td>\n      <td>['red', 'red', 'red &amp; white', 'black', 'red', ...</td>\n      <td>other</td>\n      <td>708</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>393221000</td>\n      <td>is the sky blue</td>\n      <td>is the</td>\n      <td>yes</td>\n      <td>['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...</td>\n      <td>yes/no</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>393221001</td>\n      <td>is there snow on the mountains</td>\n      <td>is there</td>\n      <td>yes</td>\n      <td>['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...</td>\n      <td>yes/no</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>96886</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>393195000</td>\n      <td>why is she smiling</td>\n      <td>why</td>\n      <td>happy</td>\n      <td>['heard joke', 'happy', \"she's enjoying her bo...</td>\n      <td>other</td>\n      <td>419</td>\n    </tr>\n    <tr>\n      <th>96887</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>393195001</td>\n      <td>is the woman wearing a hat</td>\n      <td>is the woman</td>\n      <td>yes</td>\n      <td>['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...</td>\n      <td>yes/no</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>96888</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>393195002</td>\n      <td>is the woman wearing glasses</td>\n      <td>is the woman</td>\n      <td>yes</td>\n      <td>['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...</td>\n      <td>yes/no</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>96889</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>262136000</td>\n      <td>can he be sharpening blades</td>\n      <td>none of the above</td>\n      <td>yes</td>\n      <td>['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'ye...</td>\n      <td>yes/no</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>96890</th>\n      <td>/kaggle/input/visual-question-answering/train2...</td>\n      <td>262136002</td>\n      <td>is this a normal thing to see in recent history</td>\n      <td>is this a</td>\n      <td>no</td>\n      <td>['no', 'no', 'no', 'yes', 'no', 'no', 'no', 'n...</td>\n      <td>yes/no</td>\n      <td>559</td>\n    </tr>\n  </tbody>\n</table>\n<p>96891 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import BertTokenizer, AutoImageProcessor\n\nclass VQADataset(Dataset):\n    def __init__(self, dataframe, bert_tokenizer, image_processor):\n        self.dataframe = dataframe\n        self.bert_tokenizer = bert_tokenizer\n        self.image_processor = image_processor\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        \n        # Process the text\n        text_inputs = self.bert_tokenizer(\n            row['question_preprocessed'],\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Process the image\n        image = Image.open(row['image_id']).convert(\"RGB\")\n        image_inputs = self.image_processor(images=image, return_tensors='pt')\n        \n        # Prepare the label\n        label = torch.tensor(row['answer_encoded'], dtype=torch.long)\n        \n        # Merge the inputs\n        inputs = {\n            'input_ids': text_inputs['input_ids'].squeeze(0),\n            'attention_mask': text_inputs['attention_mask'].squeeze(0),\n            'pixel_values': image_inputs['pixel_values'].squeeze(0),\n            'labels': label\n        }\n        return inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:16.100133Z","iopub.execute_input":"2024-05-16T05:15:16.100590Z","iopub.status.idle":"2024-05-16T05:15:27.750265Z","shell.execute_reply.started":"2024-05-16T05:15:16.100548Z","shell.execute_reply":"2024-05-16T05:15:27.749359Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-16 05:15:18.478556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-16 05:15:18.478685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-16 05:15:18.622607: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df, test_df = train_test_split(final_train_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:27.751734Z","iopub.execute_input":"2024-05-16T05:15:27.752052Z","iopub.status.idle":"2024-05-16T05:15:27.790448Z","shell.execute_reply.started":"2024-05-16T05:15:27.752026Z","shell.execute_reply":"2024-05-16T05:15:27.789514Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(len(train_df), len(test_df))","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:27.791762Z","iopub.execute_input":"2024-05-16T05:15:27.792066Z","iopub.status.idle":"2024-05-16T05:15:27.797044Z","shell.execute_reply.started":"2024-05-16T05:15:27.792041Z","shell.execute_reply":"2024-05-16T05:15:27.796037Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"77512 19379\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\nimage_processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n# Create a dataset instance\ntrain_dataset = VQADataset(train_df, bert_tokenizer, image_processor)\neval_dataset = VQADataset(test_df, bert_tokenizer, image_processor)\n\n# Create a DataLoader instance\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\ntest_dataloader = DataLoader(eval_dataset, batch_size=2, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:27.798861Z","iopub.execute_input":"2024-05-16T05:15:27.799243Z","iopub.status.idle":"2024-05-16T05:15:29.614066Z","shell.execute_reply.started":"2024-05-16T05:15:27.799209Z","shell.execute_reply":"2024-05-16T05:15:29.613229Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d88e2999c624aa0aa359648e22d5c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c01de1b4fc45239f8957b34cba85dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a42ea607f1914a31947a8e18dc11588c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf15a298e84418da231f78b64b93e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8530abf42dd46068915ccc416a2ab04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e628d19e4c47cfbf597963be6e663f"}},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import BertModel, ViTModel\n\nclass VQAModel(nn.Module):\n    def __init__(self, bert_model_name='google-bert/bert-base-uncased', vit_model_name='google/vit-base-patch16-224', num_labels=1000, intermediate_dim=128):\n        super(VQAModel, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.vit = ViTModel.from_pretrained(vit_model_name)\n        self.fusion = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size + self.vit.config.hidden_size, intermediate_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n        )\n        \n        self.classifier = nn.Linear(intermediate_dim, num_labels)\n        # self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n        # Get BERT and ViT outputs\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        vit_outputs = self.vit(pixel_values=pixel_values)\n        \n        fused_output = self.fusion(\n            torch.cat(\n                [\n                    bert_outputs.pooler_output,\n                    vit_outputs.pooler_output,\n                ],\n                dim=1\n            )\n        )\n        \n        logits = self.classifier(fused_output)\n        \n        # Element-wise dot product of BERT and ViT embeddings\n#         combined = bert_outputs.last_hidden_state[:, 0, :] * vit_outputs.pooler_output\n        \n        # Classification\n#         logits = self.classifier(combined)\n        \n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n            return loss, logits\n        else:\n            return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:29.615276Z","iopub.execute_input":"2024-05-16T05:15:29.616271Z","iopub.status.idle":"2024-05-16T05:15:30.003547Z","shell.execute_reply.started":"2024-05-16T05:15:29.616232Z","shell.execute_reply":"2024-05-16T05:15:30.002393Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n    logits, labels = eval_tuple\n    preds = logits.argmax(axis=-1)\n    return {\n        \"acc\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n    }","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:30.005156Z","iopub.execute_input":"2024-05-16T05:15:30.005951Z","iopub.status.idle":"2024-05-16T05:15:30.012829Z","shell.execute_reply.started":"2024-05-16T05:15:30.005909Z","shell.execute_reply":"2024-05-16T05:15:30.011732Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nmodel = VQAModel()\nmodel.to(device)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working',\n    num_train_epochs=2,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='/kaggle/working',\n    save_steps=19378,\n    save_total_limit=2,\n    logging_steps=19378,\n    report_to=[],  # Disable logging to wandb\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,  # assuming you have a validation dataset\n    compute_metrics=compute_metrics,  # You can define metrics if needed\n)\n\n# Track time taken for training\nstart_time = time.time()\ntrain_result = trainer.train()\nend_time = time.time()\n\ntrain_time = end_time - start_time\nprint(f\"Training time: {train_time} seconds\")\n\n# # Save the final model\n# trainer.save_model('/kaggle/working/vqa_model')\n\n# # Save the training arguments\n# training_args.save('/kaggle/working/vqa_model/training_args.bin')\n\n# for epoch in range(training_args.num_train_epochs):\n#     print(f\"Epoch {epoch}, Loss: {compute_loss(model, train_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:15:30.016706Z","iopub.execute_input":"2024-05-16T05:15:30.017130Z","iopub.status.idle":"2024-05-16T05:16:31.306559Z","shell.execute_reply.started":"2024-05-16T05:15:30.017095Z","shell.execute_reply":"2024-05-16T05:16:31.304885Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5a2a4ee05b74857bf82605556fc345d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b808e074b3ad44e588f55ec46747d8dc"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='77512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    6/77512 00:35 < 191:27:07, 0.11 it/s, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Track time taken for training\u001b[39;00m\n\u001b[1;32m     31\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 32\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m train_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"eval_result = trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:16:31.307731Z","iopub.status.idle":"2024-05-16T05:16:31.308301Z","shell.execute_reply.started":"2024-05-16T05:16:31.308025Z","shell.execute_reply":"2024-05-16T05:16:31.308047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Evaluation Metrics: {eval_result}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:16:31.309501Z","iopub.status.idle":"2024-05-16T05:16:31.309885Z","shell.execute_reply.started":"2024-05-16T05:16:31.309691Z","shell.execute_reply":"2024-05-16T05:16:31.309706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training Metrics: {train_result.metrics}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:16:31.311639Z","iopub.status.idle":"2024-05-16T05:16:31.312011Z","shell.execute_reply.started":"2024-05-16T05:16:31.311831Z","shell.execute_reply":"2024-05-16T05:16:31.311846Z"},"trusted":true},"execution_count":null,"outputs":[]}]}